{"cells":[{"cell_type":"code","source":["%scala\n%md #### Basic Interaction with Scala"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nprintln(\"Hello from spark scala shell\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hello from spark scala shell\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["%scala\nval ages = Array(20,31,40,50)\nages.foreach(println)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">20\n31\n40\n50\nages: Array[Int] = Array(20, 31, 40, 50)\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["%scala\n\ndef isOddAge(age:Int) : Boolean ={(age % 2) == 1}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">isOddAge: (age: Int)Boolean\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["%scala\nages.filter(age => isOddAge(age)).foreach(println)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">31\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["%scala\nspark.version"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res25: String = 2.4.5\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["%scala\nspark.conf.getAll.foreach(println)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(spark.r.sql.derby.temp.dir,/tmp/RtmptgcuWT)\n(spark.databricks.clusterUsageTags.clusterNoDriverDaemon,false)\n(spark.databricks.delta.multiClusterWrites.enabled,true)\n(spark.databricks.clusterUsageTags.cloudProvider,AWS)\n(spark.hadoop.fs.abfs.impl.disable.cache,true)\n(spark.databricks.delta.logStore.crossCloud.fatal,true)\n(spark.databricks.clusterUsageTags.hailEnabled,false)\n(spark.hadoop.fs.s3a.fast.upload.default,true)\n(spark.sql.ui.retainedExecutions,100)\n(spark.sql.warehouse.dir,/user/hive/warehouse)\n(spark.speculation,false)\n(spark.hadoop.databricks.dbfs.client.version,v1)\n(spark.hadoop.hive.server2.use.SSL,true)\n(eventLog.rolloverIntervalSeconds,3600)\n(spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType,ebs_volume_type: GENERAL_PURPOSE_SSD\n)\n(spark.executor.extraJavaOptions,-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1)\n(spark.driver.host,10.172.230.13)\n(spark.sql.hive.convertMetastoreParquet,true)\n(spark.scheduler.listenerbus.eventqueue.capacity,20000)\n(spark.serializer.objectStreamReset,100)\n(spark.databricks.clusterUsageTags.instanceBootstrapType,ssh)\n(spark.databricks.session.share,false)\n(spark.databricks.passthrough.adls.gen2.tokenProviderClassName,com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider)\n(spark.databricks.clusterUsageTags.driverInstancePrivateIp,10.172.243.44)\n(spark.eventLog.enabled,false)\n(spark.r.backendConnectionTimeout,604800)\n(spark.hadoop.fs.s3a.multipart.threshold,104857600)\n(spark.shuffle.manager,SORT)\n(spark.hadoop.fs.wasb.impl,shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem)\n(spark.streaming.driver.writeAheadLog.closeFileAfterWrite,true)\n(spark.databricks.clusterUsageTags.clusterEbsVolumeSize,0)\n(spark.hadoop.fs.wasbs.impl.disable.cache,true)\n(spark.cleaner.referenceTracking.blocking,false)\n(spark.databricks.clusterUsageTags.region,us-west-2)\n(spark.driver.maxResultSize,4g)\n(spark.metrics.conf,/databricks/spark/conf/metrics.properties)\n(spark.hadoop.fs.s3.impl,com.databricks.s3a.S3AFileSystem)\n(spark.databricks.clusterUsageTags.clusterSku,STANDARD_SKU)\n(spark.databricks.clusterUsageTags.clusterOwnerOrgId,8477591678277525)\n(spark.databricks.clusterUsageTags.autoTerminationMinutes,120)\n(spark.databricks.clusterUsageTags.clusterGeneration,0)\n(spark.hadoop.fs.adl.impl,com.databricks.adl.AdlFileSystem)\n(spark.databricks.clusterUsageTags.enableElasticDisk,false)\n(spark.hadoop.fs.abfss.impl.disable.cache,true)\n(spark.databricks.clusterUsageTags.enableCredentialPassthrough,false)\n(spark.ui.port,44138)\n(spark.hadoop.spark.driverproxy.customHeadersToProperties,X-Databricks-User-Token:spark.databricks.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds)\n(spark.driver.port,40519)\n(spark.shuffle.reduceLocality.enabled,false)\n(spark.shuffle.service.enabled,true)\n(spark.databricks.acl.provider,com.databricks.sql.acl.ReflectionBackedAclProvider)\n(spark.databricks.eventLog.dir,eventlogs)\n(spark.extraListeners,com.databricks.backend.daemon.driver.DBCEventLoggingListener)\n(spark.hadoop.hive.server2.keystore.password,gb1gQqZ9ZIHS)\n(spark.rdd.compress,true)\n(spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount,0)\n(spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb,0)\n(spark.databricks.passthrough.glue.credentialsProviderFactoryClassName,com.databricks.backend.daemon.driver.aws.DatabricksCredentialProviderFactory)\n(spark.databricks.redactor,com.databricks.spark.util.DatabricksSparkLogRedactorProxy)\n(spark.databricks.clusterUsageTags.driverInstanceId,i-04d1a17104da84e9e)\n(spark.hadoop.parquet.memory.pool.ratio,0.5)\n(spark.databricks.clusterUsageTags.clusterName,Beginning Apache Spark)\n(spark.repl.class.uri,spark://10.172.230.13:40519/classes)\n(spark.databricks.clusterUsageTags.clusterFirstOnDemand,0)\n(spark.driver.tempDirectory,/local_disk0/tmp)\n(spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class,com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory)\n(spark.hadoop.fs.abfs.impl,shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem)\n(spark.hadoop.fs.wasbs.impl,shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem)\n(spark.databricks.clusterUsageTags.sparkVersion,6.5.x-scala2.11)\n(spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,2)\n(spark.databricks.clusterUsageTags.clusterState,Pending)\n(spark.databricks.clusterUsageTags.clusterTargetWorkers,0)\n(spark.databricks.delta.preview.enabled,true)\n(spark.repl.class.outputDir,/local_disk0/tmp/repl/spark-5251605942304540027-d974c547-4556-4548-94c6-d4d293143692)\n(spark.databricks.clusterUsageTags.workerEnvironmentId,default-worker-env)\n(spark.app.name,Databricks Shell)\n(spark.databricks.credential.redactor,com.databricks.logging.secrets.CredentialRedactorProxyImpl)\n(spark.hadoop.fs.abfss.impl,shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem)\n(spark.sql.hive.metastore.jars,/databricks/hive/*)\n(spark.hadoop.hive.warehouse.subdir.inherit.perms,false)\n(spark.sql.parquet.cacheMetadata,true)\n(spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled,false)\n(spark.hadoop.fs.s3a.multipart.size,10485760)\n(spark.storage.blockManagerTimeoutIntervalMs,300000)\n(spark.hadoop.fs.s3a.connection.maximum,200)\n(spark.r.numRBackendThreads,1)\n(spark.hadoop.parquet.block.size.row.check.min,10)\n(spark.databricks.clusterUsageTags.clusterEbsVolumeCount,0)\n(spark.scheduler.mode,FAIR)\n(spark.storage.memoryFraction,0.5)\n(spark.databricks.clusterUsageTags.clusterId,0506-165928-tipsy712)\n(spark.databricks.clusterSource,UI)\n(spark.databricks.passthrough.adls.tokenProviderClassName,com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider)\n(spark.sql.hive.convertCTAS,true)\n(spark.databricks.clusterUsageTags.clusterMetastoreAccessType,RDS_DIRECT)\n(spark.databricks.overrideDefaultCommitProtocol,org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol)\n(spark.files.fetchFailure.unRegisterOutputOnHost,true)\n(spark.logConf,true)\n(spark.hadoop.parquet.block.size.row.check.max,10)\n(spark.databricks.clusterUsageTags.clusterCreator,Webapp)\n(spark.databricks.tahoe.logStore.class,com.databricks.tahoe.store.DelegatingLogStore)\n(spark.executor.tempDirectory,/local_disk0/tmp)\n(spark.databricks.clusterUsageTags.clusterSpotBidPricePercent,100)\n(spark.databricks.sparkContextId,5251605942304540027)\n(spark.worker.cleanup.enabled,false)\n(spark.databricks.driverNodeTypeId,dev-tier-node)\n(spark.hadoop.hive.server2.idle.operation.timeout,7200000)\n(spark.speculation.multiplier,3)\n(spark.task.reaper.enabled,true)\n(spark.databricks.workspace.matplotlibInline.enabled,true)\n(spark.databricks.clusterUsageTags.containerZoneId,us-west-2c)\n(spark.hadoop.hive.server2.session.check.interval,60000)\n(spark.databricks.clusterUsageTags.driverContainerPrivateIp,10.172.230.13)\n(spark.sql.streaming.checkpointFileManagerClass,com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager)\n(spark.databricks.clusterUsageTags.clusterEbsVolumeType,GENERAL_PURPOSE_SSD)\n(spark.sql.streaming.stopTimeout,15s)\n(spark.executor.id,driver)\n(spark.hadoop.databricks.s3commit.client.sslTrustAll,false)\n(spark.hadoop.hive.server2.idle.session.timeout,900000)\n(spark.databricks.clusterUsageTags.clusterPinned,false)\n(spark.hadoop.hive.server2.thrift.http.port,10000)\n(spark.databricks.clusterUsageTags.instanceWorkerEnvId,default-worker-env)\n(spark.databricks.io.directoryCommit.enableLogicalDelete,false)\n(spark.databricks.clusterUsageTags.enableJobsAutostart,true)\n(spark.databricks.clusterUsageTags.clusterLogDestination,)\n(spark.hadoop.parquet.page.verify-checksum.enabled,true)\n(spark.databricks.clusterUsageTags.clusterResourceClass,default)\n(spark.shuffle.service.port,4048)\n(spark.databricks.passthrough.glue.executorServiceFactoryClassName,com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory)\n(spark.databricks.acl.client,com.databricks.spark.sql.acl.client.SparkSqlAclClient)\n(spark.databricks.clusterUsageTags.enableJdbcAutoStart,true)\n(spark.databricks.clusterUsageTags.clusterStateMessage,Starting Spark)\n(spark.hadoop.spark.sql.parquet.output.committer.class,org.apache.spark.sql.parquet.DirectParquetOutputCommitter)\n(spark.databricks.preemption.enabled,true)\n(spark.master,local[8])\n(spark.databricks.tahoe.logStore.aws.class,com.databricks.tahoe.store.S3LockBasedLogStore)\n(spark.hadoop.fs.adl.impl.disable.cache,true)\n(spark.rpc.message.maxSize,256)\n(spark.task.reaper.killTimeout,60s)\n(spark.hadoop.hive.server2.enable.doAs,false)\n(spark.hadoop.fs.s3n.impl,com.databricks.s3a.S3AFileSystem)\n(spark.databricks.clusterUsageTags.enableLocalDiskEncryption,false)\n(spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2,0)\n(spark.hadoop.fs.azure.skip.metrics,true)\n(spark.databricks.clusterUsageTags.driverContainerId,9d3c41d997bf473fa2d92631443b856f)\n(spark.files.overwrite,true)\n(spark.hadoop.spark.thriftserver.closeSessionHeaderName,X-Databricks-SqlService-CloseSession)\n(spark.hadoop.fs.s3a.threads.max,136)\n(spark.databricks.clusterUsageTags.clusterNumSshKeys,0)\n(spark.sparkr.use.daemon,false)\n(spark.streaming.driver.writeAheadLog.allowBatching,true)\n(spark.databricks.clusterUsageTags.clusterScalingType,fixed_size)\n(spark.databricks.workerNodeTypeId,dev-tier-node)\n(spark.sql.sources.commitProtocolClass,com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol)\n(spark.databricks.clusterUsageTags.clusterWorkers,0)\n(spark.sql.hive.metastore.sharedPrefixes,org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks)\n(spark.executor.memory,8278m)\n(spark.databricks.clusterUsageTags.dataPlaneRegion,us-west-2)\n(spark.home,/databricks/spark)\n(spark.databricks.tahoe.logStore.azure.class,com.databricks.tahoe.store.AzureLogStore)\n(spark.hadoop.parquet.page.size.check.estimate,false)\n(spark.executor.extraClassPath,/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/api-base--api-base_java-spark_2.4_2.11_deploy.jar:/databricks/jars/api-base--api-base-spark_2.4_2.11_deploy.jar:/databricks/jars/api--rpc--rpc_parser-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--endpoints-spark_2.4_2.11_deploy.jar:/databricks/jars/chauffeur-api--chauffeur-api-spark_2.4_2.11_deploy.jar:/databricks/jars/common--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/common--credentials--credentials-spark_2.4_2.11_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:/databricks/jars/common--jetty--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/common--lazy--lazy-spark_2.4_2.11_deploy.jar:/databricks/jars/common--libcommon_resources.jar:/databricks/jars/common--path--path-spark_2.4_2.11_deploy.jar:/databricks/jars/common--rate-limiter--rate-limiter-spark_2.4_2.11_deploy.jar:/databricks/jars/common--storage--storage-spark_2.4_2.11_deploy.jar:/databricks/jars/common--tracing--tracing-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--client--utils-spark_2.4_2.11_deploy.jar:/databricks/jars/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:/databricks/jars/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--extern-spark_2.4_2.11_deploy.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-client-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-common_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive1_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-hive2_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-shim-loader_deploy.jar:/databricks/jars/----glue-catalog-spark-client--glue-catalog-spark-client_deploy.jar:/databricks/jars/----jackson_annotations_shaded--libjackson-annotations.jar:/databricks/jars/----jackson_core_shaded--libjackson-core.jar:/databricks/jars/----jackson_databind_shaded--libjackson-databind.jar:/databricks/jars/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:/databricks/jars/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--managedLibraries--managedLibraries-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--api--typemappers--typemappers-spark_2.4_2.11_deploy.jar:/databricks/jars/libraries--libraries-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:/databricks/jars/logging--utils--logging-utils-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--client--client-spark_2.4_2.11_deploy.jar:/databricks/jars/s3commit--common--common-spark_2.4_2.11_deploy.jar:/databricks/jars/s3--s3-spark_2.4_2.11_deploy.jar:/databricks/jars/----scalapb_090--com.fasterxml.jackson.core__jackson-core__2.9.9_shaded.jar:/databricks/jars/----scalapb_090--com.google.android__annotations__4.1.1.4_shaded.jar:/databricks/jars/----scalapb_090--com.google.api.grpc__proto-google-common-protos__1.12.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.code.gson__gson__2.7_shaded.jar:/databricks/jars/----scalapb_090--com.google.errorprone__error_prone_annotations__2.3.2_shaded.jar:/databricks/jars/----scalapb_090--com.google.guava__guava__20.0_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.google.protobuf__protobuf-java-util__3.7.1_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__fastparse_2.11__2.1.2_shaded.jar:/databricks/jars/----scalapb_090--com.lihaoyi__sourcecode_2.11__0.1.6_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-api__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-context__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-core__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-netty__1.22.1_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-protobuf-lite__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.grpc__grpc-stub__1.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-api__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.opencensus__opencensus-contrib-grpc-metrics__0.21.0_shaded.jar:/databricks/jars/----scalapb_090--io.perfmark__perfmark-api__0.16.0_shaded.jar:/databricks/jars/----scalapb_090--org.codehaus.mojo__animal-sniffer-annotations__1.17_shaded.jar:/databricks/jars/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.11_deploy_shaded.jar:/databricks/jars/secret-manager--api--api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--common--spark-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--common-utils--utils-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--conf-reader--conf-reader_lib-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--antlr--parser-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--common--driver-common-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--display--display-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--driver-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--events-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--ml--ml-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--secret-redaction-spark_2.4_2.11_deploy.jar:/databricks/jars/spark--driver--spark--resources-resources.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-12679-patch_deploy.jar:/databricks/jars/spark--maven-trees--hive-exec-with-glue--hive-exec_shaded.jar:/databricks/jars/spark--maven-trees--spark_2.4--antlr--antlr--antlr__antlr__2.7.7.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.8.10.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-autoscaling--com.amazonaws__aws-java-sdk-autoscaling__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudformation--com.amazonaws__aws-java-sdk-cloudformation__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudfront--com.amazonaws__aws-java-sdk-cloudfront__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudhsm--com.amazonaws__aws-java-sdk-cloudhsm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudsearch--com.amazonaws__aws-java-sdk-cloudsearch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudtrail--com.amazonaws__aws-java-sdk-cloudtrail__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cloudwatchmetrics--com.amazonaws__aws-java-sdk-cloudwatchmetrics__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-codedeploy--com.amazonaws__aws-java-sdk-codedeploy__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitoidentity--com.amazonaws__aws-java-sdk-cognitoidentity__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-cognitosync--com.amazonaws__aws-java-sdk-cognitosync__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-datapipeline--com.amazonaws__aws-java-sdk-datapipeline__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directconnect--com.amazonaws__aws-java-sdk-directconnect__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-directory--com.amazonaws__aws-java-sdk-directory__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-dynamodb--com.amazonaws__aws-java-sdk-dynamodb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ec2--com.amazonaws__aws-java-sdk-ec2__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-efs--com.amazonaws__aws-java-sdk-efs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticbeanstalk--com.amazonaws__aws-java-sdk-elasticbeanstalk__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elasticloadbalancing--com.amazonaws__aws-java-sdk-elasticloadbalancing__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-elastictranscoder--com.amazonaws__aws-java-sdk-elastictranscoder__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-emr--com.amazonaws__aws-java-sdk-emr__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glacier--com.amazonaws__aws-java-sdk-glacier__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-glue--com.amazonaws__aws-java-sdk-glue__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-iam--com.amazonaws__aws-java-sdk-iam__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-importexport--com.amazonaws__aws-java-sdk-importexport__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kinesis--com.amazonaws__aws-java-sdk-kinesis__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-lambda--com.amazonaws__aws-java-sdk-lambda__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-logs--com.amazonaws__aws-java-sdk-logs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-machinelearning--com.amazonaws__aws-java-sdk-machinelearning__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-opsworks--com.amazonaws__aws-java-sdk-opsworks__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-rds--com.amazonaws__aws-java-sdk-rds__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-redshift--com.amazonaws__aws-java-sdk-redshift__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-route53--com.amazonaws__aws-java-sdk-route53__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ses--com.amazonaws__aws-java-sdk-ses__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-simpleworkflow--com.amazonaws__aws-java-sdk-simpleworkflow__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sqs--com.amazonaws__aws-java-sdk-sqs__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-ssm--com.amazonaws__aws-java-sdk-ssm__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-storagegateway--com.amazonaws__aws-java-sdk-storagegateway__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-support--com.amazonaws__aws-java-sdk-support__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-swf-libraries--com.amazonaws__aws-java-sdk-swf-libraries__1.11.22.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-workspaces--com.amazonaws__aws-java-sdk-workspaces__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.595.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.carrotsearch--hppc--com.carrotsearch__hppc__0.7.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.chuusai--shapeless_2.11--com.chuusai__shapeless_2.11__2.3.2.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.7.0.jar:/databricks/jars/spark--maven-trees--spark_2.4--com.databricks--dbml-local_2.11--com.databricks__dbml-local_2.11__0.5.0-db8-spark2.4.jar:/databricks/jars/spark--ma\n*** WARNING: skipped 52374 bytes of output ***\n\n(spark.sql.catalogImplementation,hive)\n(spark.databricks.passthrough.s3a.tokenProviderClassName,com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider)\n(spark.databricks.clusterUsageTags.isSingleUserCluster,false)\n(spark.hadoop.hive.server2.keystore.path,/databricks/keys/jetty-ssl-driver-keystore.jks)\n(spark.hadoop.fs.wasb.impl.disable.cache,true)\n(spark.hadoop.hive.server2.transport.mode,http)\n(spark.driver.allowMultipleContexts,false)\n(spark.databricks.clusterUsageTags.enableSqlAclsOnly,false)\n(spark.databricks.clusterUsageTags.clusterNodeType,dev-tier-node)\n(spark.databricks.clusterUsageTags.containerType,LXC)\n(spark.databricks.clusterUsageTags.driverPublicDns,ec2-52-11-174-184.us-west-2.compute.amazonaws.com)\n(spark.databricks.sqlservice.querylog.client,com.databricks.spark.sqlservice.client.DriverToSqlServiceQueryLogClient)\n(spark.files.useFetchCache,false)\n(spark.sql.allowMultipleContexts,false)\n(spark.databricks.clusterUsageTags.enableDfAcls,false)\n(spark.databricks.clusterUsageTags.clusterPythonVersion,2)\n(spark.hadoop.fs.s3a.impl,com.databricks.s3a.S3AFileSystem)\n(spark.shuffle.memoryFraction,0.2)\n(spark.databricks.clusterUsageTags.clusterAvailability,ON_DEMAND)\n(spark.sql.parquet.compression.codec,snappy)\n(spark.hadoop.mapred.output.committer.class,com.databricks.backend.daemon.data.client.DirectOutputCommitter)\n(spark.hadoop.spark.sql.sources.outputCommitterClass,com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter)\n(spark.app.id,local-1588784403755)\n(spark.databricks.clusterUsageTags.clusterOwnerUserId,906624470951774)\n(spark.hadoop.fs.s3a.fast.upload,true)\n(spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled,false)\n(spark.databricks.clusterUsageTags.driverNodeType,dev-tier-node)\n(spark.speculation.quantile,0.9)\n(spark.databricks.cloudProvider,AWS)\n(spark.akka.frameSize,256)\n(spark.hadoop.parquet.page.write-checksum.enabled,true)\n(spark.databricks.clusterUsageTags.clusterAllTags,[{&quot;key&quot;:&quot;Name&quot;,&quot;value&quot;:&quot;ce1-worker&quot;}])\n(spark.sql.hive.metastore.version,0.13.0)\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["%scala\n%md ####Resilient Ditributed Datasets"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval stringList = Array(\"Spark is awesome\",\"Spark is cool\")\nval stringRDD = spark.sparkContext.parallelize(stringList)\nval allCapsRDD= stringRDD.map(line => line.toUpperCase)\nallCapsRDD.collect().foreach(println)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SPARK IS AWESOME\nSPARK IS COOL\nstringList: Array[String] = Array(Spark is awesome, Spark is cool)\nstringRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at command-2838462046687783:2\nallCapsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at map at command-2838462046687783:3\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"Scala and Spark|Interactions","notebookId":2838462046687774},"nbformat":4,"nbformat_minor":0}
